

<!doctype html>
<html>

<head>


<title>Keji He</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Keji He, 何科技, CRIPAC, NLPR, CASIA, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, New Laboratory of Pattern Recognition, 人工智能全国重点实验室, State Key Laboratory of Multimodal Artificial Intelligence Systems"> 
<meta name="description" content="Keji He's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Keji He 何科技<h1>
				</div>

                <h3>Assistant Professor</h3>

		<p>
                    School of Artificial Intelligence </br>
                    Shandong University </br>
					</br>
                    Email: keji.he@sdu.edu.cn </br>
		</p>
		<p>
			<!-- <a href="https://github.com/hongyuanyu"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp; -->
			<a href="https://scholar.google.com/citations?user=RHPI-NQAAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
			<!-- <a href="https://www.linkedin.com/in/hongyuan-yu-726126178/"><img src="assets/logos/linkedin_logo.png" height="30px"></a>&nbsp;&nbsp; -->
		</p>
			</td>

			</td>
			<td width="25%">
				<img src="assets/imgs/hongyuan.jpeg" width="100%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>News</h2>
<ul>   
<li> <p>2024.12, One paper was published on NeurIPS 2024.</a> </p></li> 
<li> <p>2024.06, We won the CVPR2024 NTIRE Challenge on Efficient Super-Resolution competition (2 champions) and the related paper has been accepted by the CVPR2024 NTIRE.</a> </p></li> 
</ul>

<h2>Biography</h2> 
<p>
Dr. Keji He is an assistant professor at the School of Artificial Intelligence, Shandong University (SDU). He finished his PhD at the Institute of Automation, Chinese Academy of Sciences, supervised by Prof. <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN">Liang Wang</a>  (IEEE Fellow) and Prof. <a href="https://yanrockhuang.github.io/">Yan Huang</a>. He is also a jointly PhD at the National University of Singapore (NUS), supervised by Prof. <a href="https://scholar.google.com/citations?user=w69Buq0AAAAJ&hl=en">Xinchao Wang</a>.
</p>

<p>
His research interests include vision-and-language understanding, generative AI, and embodied AI.
</p>

<h2>Join Us</h2> 
<p>
I am actively seeking self-motivated Master students(MSC)/Research Assistants(RA) in AI.<br>
The research topics mainly include but are not limited to the following:
<ul>
  <li>Vision-and-Language Understanding</li>
  <li>Embodied AI</li>
  <li>Multimodal Large Language Model</li>
  <li>Large Language Model</li>
  <li>AI Safety</li>
</ul>
If you’re interested, please feel free to reach out via email (keji.he@sdu.edu.cn) with your curriculum vitae.   [It's not where you come from, it's what you do that matters.].
</p>
						

<h2> Selected Publications</h2> 
<ul>

<li><p><strong>Keji He*</strong>, Kehan Chen*, Jiawang Bai, Yan Huang, Qi Wu, Shu-Tao Xia, and Liang Wang, Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor, <i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, 2024. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/58e6c003c9fb3992265005ff6aef1913-Paper-Conference.pdf" target="_self">PDF</a> </p></li>
<li><p><strong>Keji He</strong>, Ya Jing, Yan Huang, Zhihe Lu, Dong An, and Liang Wang, Memory-Adaptive Vision-and-Language Navigation, <i>Pattern Recognition (<strong>PR</strong>)</i>, 2024. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324002620" target="_self">PDF</a> </p></li>
<li><p>Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, <strong>Keji He</strong>, and Liang Wang, Etpnav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments, <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>)</i>, 2024. <a href="https://arxiv.org/pdf/2304.03047" target="_self">PDF</a> </p></li>
<li><p><strong>Keji He</strong>, Chenyang Si, Zhihe Lu, Yan Huang, Liang Wang, and Xinchao Wang, Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation, <i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, 2023. <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf" target="_self">PDF</a> </p></li>
<li><p><strong>Keji He</strong>, Yan Huang, Qi Wu, Jianhua Yang, Dong An, Shuanglin Sima, and Liang Wang, Landmark-Rxr: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision, <i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, 2024. <a href="https://proceedings.neurips.cc/paper/2021/file/0602940f23884f782058efac46f64b0f-Paper.pdf" target="_self">PDF</a> </p></li>
</ul>
  

<h2>Competitions</h2> 
<ul>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ntire.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> CVPR2024 New Trends in Image Restoration and Enhancement workshop and associated challenges (NTIRE). 
      Our team (<b>Hongyuan Yu</b>, Wan Cheng, Yuxin Hong, Binnan Han, Zhuoyuan Wu, Yajun Zou, Yuqing Liu, Jizhe Li, Keji He, Chao Fan, Heng Zhang, Xiaolin Zhang, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">champion</font> of Efficient Super-Resolution competition (Main-Track and Runtime). 
      Our team (<b>Hongyuan Yu</b>, Wan Cheng, Yuxin Hong, Zhijuan Huang, Yajun Zou, Yuan Huang, Jiamin Lin, Xianyu Guan, Yongsheng Yu, Doan Zhang, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">champion</font> of Image Super-Resolution (x4) competition. 
      Our team Zhijuan Huang, (<b>Hongyuan Yu</b>, Cheng Wan, Wending Xiang, Jiamin Lin, Hang Zhong, Qiangsong Zhang, Yue Sun, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">runner-up</font> of RAW Image Super-Resolution. 
      Our team Binnan Han, (<b>Hongyuan Yu</b>, Wuzhuo Yuan, Wan Cheng, Yuqing Liu, Haodong Yu, Jizhe Li, Zhijuan Huang, Yajun Zou, Yuan Huang, Jiamin Lin, Xianyu Guan, Qi Jia, Heng Zhang, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">runner-up</font> of CVPR2024 AIS on Real-time Compressed Image Super-Resolution competition.
       See details here: <a href="https://cvlai.net/ntire/2024/">Results</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/mipi.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> CVPR2023 Mobile intelligent photography and imaging (MIPI) Challenge. Our team (<b>Hongyuan Yu</b>, Yuqing Liu, Weichen Yu, Lin Ge, Binnan Han, Xiaolin Zhang, Zhen Dong, Xuanwu Yin, Kunlong Zuo)
       is the <font color="#FF0000">champion</font> of RGBW Sensor Fusion competition and <font color="#FF0000">champion</font> of RGBW Sensor Re-mosaic competition. See details here: <a href="https://mipi-challenge.org/MIPI2023/">Results</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ijcai_challenge.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 1st Learning and Mining with Noisy Labels Challenge. Our team (Weichen Yu, <b>Hongyuan Yu</b>, Yan Huang, Dong An,
      Keji He, Zhipeng Zhang, Xiuchuan Li, Liang Wang) is the <font color="#FF0000">runner-up</font> of task 1-1 and <font color="#FF0000">2nd runner-up</font> of task 1-2. See details here: <a href="https://yuankaiqi.github.io/REVERIE_Chalhttp://ucsc-real.soe.ucsc.edu:1995/Competition.html">Results</a>.
    </p>
    </td></tr></table>

<table class="imgtable"><tr><td>
  <img src="assets/logos/csig.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
  <td align="left">
  <p> REVERIE Challenge 2022. Our team TouchFish (Dong An, Yifeng Su, Shuanglin Sima, <b>Hongyuan Yu</b>, Weichen Yu, Yan Huang) is the <font color="#FF0000">runner-up</font> of both channels. See details here: <a href="https://yuankaiqi.github.io/REVERIE_Challenge/challenge_2022.html">Results of REVERIE Challenge 2022</a>.
  </p>
  </td></tr></table>

<table class="imgtable"><tr><td>
<img src="assets/logos/vot.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
<td align="left">
<p> VOT 2019: Visual Object Tracking Challenge. Our team (<b>Hongyuan Yu</b>, Houwen Peng, Zhirong Wu, Yan Huang,
Jianlong Fu, Liang Wang) is the <font color="#FF0000">champion</font> of the task: RGB-D. See details here: <a href="https://data.votchallenge.net/vot2019/presentations/vot2019_rgbd.pdf">Results of VOT 2019</a>.
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
  <img src="assets/logos/robocup.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
  <td align="left">
  <p> ROBOCUP JAPAN OPEN 2016. Our team ( Zhengdong Luo, Zihao An, Chengguang Xu, Fengting Li, Shuning Han, Yuanyuan Tong, Yanmei Jiao, <b>Hongyuan Yu</b>, Xiaotang Du) is the <font color="#FF0000">champion</font> of the task: Home and Simulation, the <font color="#FF0000">runner-up</font> of the task: Education. See details here: <a href="https://cc.nankai.edu.cn/2016/0405/c13291a147168/page.htm">Results of ROBOCUP JAPAN OPEN 2016</a>.
  </p>
  </td></tr></table>

</ul> 
  
  
<h2> Professional Activities</h2> 
<ul>
<li><p>Valedictorian, Institute of Automation Chinese Academy of Sciences (2022)</a></p></li>
<li><p>Chair, IEEE Student Branch, University of Chinese Academy of Sciences (2019-2020)</a></p></li>
<li><p>Reviewer, TPAMI, IJCV, CVPR, ECCV, ICML, ICLR, NeurIPS, AAAI, ACM MM, PR, ICIG, ICME, etc</p></li>
</ul>
  
  
<h2> Honors and Awards</h2> 
<ul> 
<li><p>2022, Runner-up of Learning and Mining with Noisy Labels Challenge (IJCAI-ECAI)</p> </li>
<li><p>2022, Runner-up of REVERIE Challenge (CSIG)</p> </li>
<li><p>2022, <font color="#FF0000">Valedictorian</font> of Institute of Automation Chinese Academy of Sciences</p> </li>
<li><p>2022, 中科院院长奖</p> </li>
<li><p>2022, 北京市优秀毕业生</p> </li>
<li><p>2021, 中国科学院自动化研究所攀登一等奖学金</p> </li>
<li><p>2020, 中国科学院大学IEEE学生分会主席贡献奖</p> </li>
<li><p>2019, <font color="#FF0000">Champion</font> of RGBD Object Tracking Challenge in the Workshop on VOT, ICCV 2019</p> </li>
<li><p>2019, 中国科学院大学三好学生</p> </li>
<li><p>2017, 中国科学院自动化研究所新生奖学金</p> </li>
<li><p>2016, Runner-up of ROBCUP JAPAN OPEN, 2016 </p> </li>
<li><p>2016, 南开大学公能奖学金 </p> </li>
<li><p>2016, 华为杯智能设计大赛三等奖 </p> </li>
<li><p>2015, Honorable Mention of MCM 2015 </p> </li>
<li><p>2014-2016, 国家励志奖学金</p> </li>
<li><p>2014-2016, 南开大学优秀学生干部</p> </li>

</ul>


<table width="100%"> 
	<tr> 
		<td align="center">&copy; Hongyuan Yu | Last update: Aug 2023</td>
	</tr> 
</table>

</div>


</body>

</html>

